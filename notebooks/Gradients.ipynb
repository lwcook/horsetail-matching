{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we look at how to use the gradient of the horsetail matching metric to speed up optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from horsetailmatching import UncertainParameter, HorsetailMatching\n",
    "from horsetailmatching.demoproblems import TP1, TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the purely probabilistic case and a simple test problem. We set up the uncertain parameters and create the horsetail matching object as usual. Note that in order to propagate gradients, we must use the kernel based method of evaluating the horsetail matching metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u1 = UncertainParameter('uniform', lower_bound=-1, upper_bound=1)\n",
    "u2 = UncertainParameter('uniform', lower_bound=-1, upper_bound=1)\n",
    "input_uncertainties = [u1, u2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horsetail matching uses the same syntax for specifying a gradient as the scipy.minimize function, through the 'jacobian' argument. If jacobian is True, then horsetail matching expects the function provided to evaluate the qoi to also return the jacobian of the qoi (the gradient with respect to the design variables) as a second parameter. Alternatively jacobian is a fuction that similarly to the qoi function takes two inputs, the values of the design variables and uncertainties, and returns the gradient. The following code demonstrates these alternatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11.711158705931167, array([-0.0401575 ,  0.04242819]))\n"
     ]
    }
   ],
   "source": [
    "fun_qjac = lambda x, u: TP2(x, u, jac=True)\n",
    "fun_q = lambda x, u: TP2(x, u, jac=False)\n",
    "fun_jac = lambda x, u: TP2(x, u, jac=True)[1]\n",
    "\n",
    "theHM = HorsetailMatching(fun_qjac, input_uncertainties, jacobian=True, method='kernel', \n",
    "                          n_samples_prob=2000, n_integration_points=1000, q_low=0, q_high=20)\n",
    "theHM = HorsetailMatching(fun_q, input_uncertainties, jacobian=fun_jac, method='kernel', \n",
    "                          n_samples_prob=2000, n_integration_points=1000)\n",
    "print(theHM.evalMetric([2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this in a gradient based optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  status: 0\n",
      " success: True\n",
      "    njev: 522\n",
      "    nfev: 522\n",
      "     fun: 9.96322479014987\n",
      "       x: array([ -4.04850338, -12.37838541])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([  9.01773800e-06,   7.55502293e-06])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "solution = minimize(theHM.evalMetric, x0=[1,1], method='CG', jac=True)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
